{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IL HW3\n",
        "## Part 2 | MAZE\n",
        "Ali Saeizadh\n",
        "810196477"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "mBMoPLmGbrIn"
      },
      "outputs": [],
      "source": [
        "from amalearn.reward import RewardBase\n",
        "from amalearn.agent import AgentBase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "YBACGmh0brIr"
      },
      "outputs": [],
      "source": [
        "from amalearn.environment import EnvironmentBase\n",
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from pprint import pprint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2 1 1 1 1 1 0 0 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 0 0 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 0 0 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 0 0 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 0 0 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 0 0 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 0 0 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 0 0 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "map_df = pd.read_csv('maze.map', header=None)\n",
        "map = map_df.to_numpy()\n",
        "print(map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "id": "pH6sNHxPbrIs"
      },
      "outputs": [],
      "source": [
        "class Environment(EnvironmentBase):\n",
        "    def __init__(self, obstacle=[], id=0, action_count=9, actionPrice=-0.01, goalReward=1000, punish=-1, j_limit=15, i_limit=15, p=0.8, container=None):\n",
        "        \"\"\"\n",
        "        initialize your variables\n",
        "        \"\"\"\n",
        "        map_df = pd.read_csv('maze.map', header=None)\n",
        "        self.map = map_df.to_numpy()\n",
        "        self.actions_name = np.array(\n",
        "            ['up', 'down', 'left', 'right', 'upright', 'upleft', 'downleft', 'dwonright', 'remain'])\n",
        "        self.actions = np.array([np.array([0, 1]), np.array([0, -1]), np.array([-1, 0]), np.array(\n",
        "            [1, 0]), np.array([1, 1]), np.array([-1, 1]), np.array([-1, -1]), np.array([1, -1]), np.array([0, 0])])\n",
        "        self.A = list(range(0, self.actions.shape[0]))\n",
        "        self.S = list(range(0, self.map.size))\n",
        "        self.i_limit = 15\n",
        "        self.j_limit = 15\n",
        "        self.p = p\n",
        "        self.goal_reward = goalReward\n",
        "        self.punish = punish\n",
        "        self.action_price = actionPrice\n",
        "        self.current_action = 0\n",
        "        self.current_state = np.array([14, 14])\n",
        "        self.agent_action_rec = []\n",
        "        self.agent_states = [self.current_state]\n",
        "\n",
        "    def isStatePossible(self, state):\n",
        "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
        "        if(state[0] >= self.i_limit or state[0] < 0):\n",
        "            return False\n",
        "        if(state[1] >= self.j_limit or state[1] < 0):\n",
        "            return False\n",
        "        if(map[state[0], state[1]] == 0):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def isAccessible(self, state, state_p):\n",
        "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
        "        if(self.isStatePossible(state_p) == False):\n",
        "            return False\n",
        "        if (np.linalg.norm(state - state_p) - np.sqrt(2) > 0):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def is_obstacle(self, state):\n",
        "        if(map[state[0], state[1]] == 0):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def number_of_accessible(self, state):\n",
        "        num = 0\n",
        "        for i in range(len(self.actions)):\n",
        "            if(self.isStatePossible(state + self.actions[i])):\n",
        "                num += 1\n",
        "        return num\n",
        "\n",
        "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
        "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
        "        if(self.isAccessible(state, state_p) == False):\n",
        "            return 0\n",
        "        if(np.array_equal(state_p - state, self.actions[action])):\n",
        "            return self.p\n",
        "        else:\n",
        "            return (1 - self.p) / self.number_of_accessible(state)\n",
        "\n",
        "    def getReward(self, state, action, state_p):\n",
        "        \"\"\"return reward of transition\"\"\"\n",
        "        if(self.isAccessible(state, state_p) == False):\n",
        "            return self.punish\n",
        "        if(self.map[state_p[0], state_p[1]] == 2):\n",
        "            return self.goal_reward + self.action_price\n",
        "        if(self.is_obstacle(state_p)):\n",
        "            return self.punish + self.action_price\n",
        "        return self.action_price\n",
        "\n",
        "    def sample_all_rewards(self):\n",
        "        return\n",
        "\n",
        "    def calculate_reward(self, action):\n",
        "        # replaced by getReward\n",
        "        return\n",
        "\n",
        "    def terminated(self):\n",
        "        S = self.available_states()\n",
        "        if(np.array_equal(self.current_state, np.array([0, 0]))):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def observe(self):\n",
        "        return {}\n",
        "\n",
        "    def available_actions(self):\n",
        "        return self.actions\n",
        "\n",
        "    def possible_actions(self, state):\n",
        "        poss_actions = np.zeros(len(self.A))\n",
        "        for a in self.A:\n",
        "            state_p = state + self.actions[a]\n",
        "            if(self.isStatePossible(state_p)):\n",
        "                poss_actions[a] = 1\n",
        "        return poss_actions\n",
        "\n",
        "    def possible_states(self, state):\n",
        "        poss_states = np.zeros(len(self.S))\n",
        "        all_states = self.available_states()\n",
        "        for s in self.S:\n",
        "            if(self.isAccessible(state, all_states[s])):\n",
        "                poss_states[s] = 1\n",
        "        return poss_states\n",
        "\n",
        "    def available_states(self):\n",
        "        states = [np.array([x, y]) for x in range(self.map.shape[0])\n",
        "                  for y in range(self.map.shape[1])]\n",
        "        counter = list(range(0, len(states)))\n",
        "        S = dict(zip(counter, states))\n",
        "        return S\n",
        "\n",
        "    def next_state(self, action):\n",
        "        choise = np.random.uniform(0, 1)\n",
        "        error_action = self.A.copy()\n",
        "        error_action.remove(action)\n",
        "        if(choise > self.p):\n",
        "            action = np.random.choice(error_action)\n",
        "\n",
        "        state_p = self.current_state + self.actions[action]\n",
        "        if(self.isAccessible(self.current_state, state_p) == False):\n",
        "            self.agent_states.append(self.current_state)\n",
        "            return [self.current_state, action]\n",
        "        if(self.is_obstacle(state_p)):\n",
        "            self.agent_states.append(self.current_state)\n",
        "            return [self.current_state, action]\n",
        "        self.current_state = state_p\n",
        "        self.agent_states.append(self.current_state)\n",
        "        return [self.current_state, action]\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = 0\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
        "        return\n",
        "\n",
        "    def step(self, action):\n",
        "        # err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
        "        # assert self.action_space.contains(action), err_msg\n",
        "        # print(self.current_state)\n",
        "        info = self.get_info(action)\n",
        "        [self.current_state, real_action] = self.next_state(action)\n",
        "        reward = self.getReward(\n",
        "            self.agent_states[-2], real_action, self.agent_states[-1])\n",
        "        done = self.terminated()\n",
        "        observation = self.observe()\n",
        "        return observation, reward, done, info, self.current_state\n",
        "\n",
        "    def close(self):\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "898Jlhsycyes"
      },
      "outputs": [],
      "source": [
        "class Agent(AgentBase):\n",
        "    def __init__(self, id, environment, discount, theta):\n",
        "        # initialize a random policy and V(s) = 0 for each state\n",
        "        self.environment = environment\n",
        "        self.mapp = {}\n",
        "        self.Q = {}\n",
        "        self.S = self.environment.available_states()\n",
        "        self.A = list(range(0, self.environment.available_actions().shape[0]))\n",
        "        # mapp states to its ids\n",
        "\n",
        "        self.V = {s: 0 for s in range(0, len(self.S))}\n",
        "        # init V\n",
        "        self.policy = {s: 0 for s in range(0, len(self.S))}\n",
        "        # init policy\n",
        "        super(Agent, self).__init__(id, environment)\n",
        "        self.discount = discount\n",
        "        self.theta = theta\n",
        "        self.current_state = len(self.S) - 1\n",
        "        self.action_rec = []\n",
        "        self.reward_rec = []\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in self.S:\n",
        "                old_V = self.V[s]\n",
        "                a = self.policy[s]\n",
        "                V_s = np.zeros(len(self.S))\n",
        "                poss_states = self.environment.possible_states(self.S[s])\n",
        "                for s_next in self.S:\n",
        "                    if(poss_states[s_next] == 1):\n",
        "                        V_s[s_next] = self.environment.getTransitionStatesAndProbs(self.S[s], a, self.S[s_next]) * (\n",
        "                            self.environment.getReward(self.S[s], a, self.S[s_next]) + self.discount * self.V[s_next])\n",
        "                self.V[s] = np.sum(V_s)\n",
        "                delta = max(delta, abs(old_V - self.V[s]))\n",
        "            if(delta < self.theta):\n",
        "                break\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        policy_stable = True\n",
        "        for s in self.S:\n",
        "            old_action = self.policy[s]\n",
        "            action_val = np.zeros(len(self.A))\n",
        "            poss_states = self.environment.possible_states(self.S[s])\n",
        "            for a in self.A:\n",
        "                V_s = np.zeros(len(self.S))\n",
        "                for s_next in self.S:\n",
        "                    if(poss_states[s_next] == 1):\n",
        "                        V_s[s_next] = self.environment.getTransitionStatesAndProbs(self.S[s], a, self.S[s_next]) * (\n",
        "                            self.environment.getReward(self.S[s], a, self.S[s_next]) + self.discount * self.V[s_next])\n",
        "                action_val[a] = np.sum(V_s)\n",
        "            action_val = (\n",
        "                (-100000 * (1 - self.environment.possible_actions(self.S[s])))) + action_val\n",
        "            self.policy[s] = np.argmax(action_val)\n",
        "            if(old_action != self.policy[s]):\n",
        "                policy_stable = False\n",
        "        return policy_stable\n",
        "\n",
        "    def value_iteration(self):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in self.S:\n",
        "                action_val = np.zeros(len(self.A))\n",
        "                old_v = self.V[s]\n",
        "                poss_states = self.environment.possible_states(self.S[s])\n",
        "                for a in self.A:\n",
        "                    V_s = np.zeros(len(self.S))\n",
        "                    for s_next in self.S:\n",
        "                        if(poss_states[s_next] == 1):\n",
        "                            V_s[s_next] = self.environment.getTransitionStatesAndProbs(self.S[s], a, self.S[s_next]) * (\n",
        "                                self.environment.getReward(self.S[s], a, self.S[s_next]) + self.discount * self.V[s_next])\n",
        "                    action_val[a] = np.sum(V_s)\n",
        "                action_val = (\n",
        "                    (-100000 * (1 - self.environment.possible_actions(self.S[s])))) + action_val\n",
        "                self.V[s] = np.max(action_val)\n",
        "                delta = max(delta, abs(old_v - self.V[s]))\n",
        "            if(delta < self.theta):\n",
        "                break\n",
        "\n",
        "        for s in self.S:\n",
        "            action_val = np.zeros(len(self.A))\n",
        "            poss_states = self.environment.possible_states(self.S[s])\n",
        "            for a in self.A:\n",
        "                V_s = np.zeros(len(self.S))\n",
        "                for s_next in self.S:\n",
        "                    if(poss_states[s_next] == 1):\n",
        "                        V_s[s_next] = self.environment.getTransitionStatesAndProbs(self.S[s], a, self.S[s_next]) * (\n",
        "                            self.environment.getReward(self.S[s], a, self.S[s_next]) + self.discount * self.V[s_next])\n",
        "                action_val[a] = np.sum(V_s)\n",
        "            action_val = (\n",
        "                (-100000 * (1 - self.environment.possible_actions(self.S[s])))) + action_val\n",
        "            self.policy[s] = np.argmax(action_val)\n",
        "\n",
        "    def policy_iteration(self):\n",
        "        while True:\n",
        "            self.policy_evaluation()\n",
        "            policy_stable = self.policy_improvement()\n",
        "            if(policy_stable):\n",
        "                break\n",
        "\n",
        "    def get_key(self, my_dict, val):\n",
        "        for key, value in my_dict.items():\n",
        "            if np.array_equal(val, value):\n",
        "                return key\n",
        "\n",
        "    # def take_action(self):\n",
        "    #     if(len(self.action_rec) < 1):\n",
        "    #         self.value_iteration()\n",
        "    #     action = self.policy[self.current_state]\n",
        "    #     self.action_rec.append(action)\n",
        "    #     obs, r, d, i, s = self.environment.step(action)\n",
        "    #     self.reward_rec.append(r)\n",
        "    #     self.environment.render()\n",
        "    #     self.current_state = self.get_key(self.S, s)\n",
        "    #     print(s)\n",
        "    #     return obs, r, d, i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent_Value_Iteration(Agent):\n",
        "    def take_action(self):\n",
        "        if(len(self.action_rec) < 1):\n",
        "            self.policy_iteration()\n",
        "        action = self.policy[self.current_state]\n",
        "        self.action_rec.append(action)\n",
        "        obs, r, d, i, s = self.environment.step(action)\n",
        "        self.reward_rec.append(r)\n",
        "        self.environment.render()\n",
        "        self.current_state = self.get_key(self.S, s)\n",
        "        return obs, r, d, i, s\n",
        "\n",
        "\n",
        "class Agent_Policy_Iteration(Agent):\n",
        "    def take_action(self):\n",
        "        if(len(self.action_rec) < 1):\n",
        "            self.value_iteration()\n",
        "        action = self.policy[self.current_state]\n",
        "        self.action_rec.append(action)\n",
        "        obs, r, d, i, s = self.environment.step(action)\n",
        "        self.reward_rec.append(r)\n",
        "        self.environment.render()\n",
        "        self.current_state = self.get_key(self.S, s)\n",
        "        return obs, r, d, i, s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {},
      "outputs": [],
      "source": [
        "trial = 100\n",
        "discount = 0.9\n",
        "theta = 0.1\n",
        "env = Environment()\n",
        "no_friction_env = Environment(obstacle=[], id=1, action_count=9, actionPrice=0,\n",
        "                              goalReward=1000, punish=-0.01, j_limit=15, i_limit=15, p=0.8, container=None)\n",
        "high_friction_env = Environment(obstacle=[], id=2, action_count=9, actionPrice=-1,\n",
        "                                goalReward=100, punish=-10, j_limit=15, i_limit=15, p=0.8, container=None)\n",
        "agent_policy_original = Agent_Policy_Iteration(1, env, discount, theta)\n",
        "agent_policy_no_friction = Agent_Policy_Iteration(\n",
        "    2, no_friction_env, discount, theta)\n",
        "agent_policy_high_friction = Agent_Policy_Iteration(\n",
        "    3, high_friction_env, discount, theta)\n",
        "agent_value = Agent_Value_Iteration(4, high_friction_env, discount, theta)\n",
        "rewrads = np.zeros([4, trial])\n",
        "trajectory = np.zeros([4, trial, 2])\n",
        "for t in range(trial):\n",
        "    obs, rewrads[0, t], d, i, trajectory[0,\n",
        "                                         t] = agent_policy_original.take_action()\n",
        "for t in range(trial):\n",
        "    obs, rewrads[1, t], d, i, trajectory[1,\n",
        "                                         t] = agent_policy_no_friction.take_action()\n",
        "for t in range(trial):\n",
        "    obs, rewrads[2, t], d, i, trajectory[2,\n",
        "                                         t] = agent_policy_high_friction.take_action()\n",
        "for t in range(trial):\n",
        "    obs, rewrads[3, t], d, i, trajectory[3, t] = agent_value.take_action()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'rewards' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_34742/2473571256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rewards' is not defined"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "env.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d5cc6a4a87dc4b3b25ba4615f4628ea484928f9572324af8a1d883d19e1da40c"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
