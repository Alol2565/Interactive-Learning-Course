{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U7yfbfrvyEwm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.5151765,  0.       ], dtype=float32)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from amalearn.agent import AgentBase\n",
        "import matplotlib.pyplot as plt\n",
        "import tiles3 as tc\n",
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "import gym\n",
        "gym.envs.register(\n",
        "    id='MountainCarMyEasyVersion-v0',\n",
        "    entry_point='gym.envs.classic_control:MountainCarEnv',\n",
        "    max_episode_steps=250,      # MountainCar-v0 uses 200\n",
        "    reward_threshold=-110.0,\n",
        ")\n",
        "env = gym.make('MountainCarMyEasyVersion-v0')\n",
        "# env = gym.make('MountainCar-v0').env\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mountain Car Tile Coder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rxnM5VdIyv_s"
      },
      "outputs": [],
      "source": [
        "env = gym.make('MountainCar-v0').env\n",
        "class MountainCarTileCoder:\n",
        "    def __init__(self, iht_size, num_tilings, num_tiles):\n",
        "        self.iht = tc.IHT(iht_size)\n",
        "        self.num_tilings = num_tilings\n",
        "        self.num_tiles = num_tiles\n",
        "    \n",
        "    def get_tiles(self, position, velocity):\n",
        "        min_position = env.min_position\n",
        "        max_position = env.max_position\n",
        "        min_velocity = -env.max_speed\n",
        "        max_velocity = env.max_speed\n",
        "        position_scale = self.num_tiles / (max_position - min_position)\n",
        "        velocity_scale = self.num_tiles / (max_velocity - min_velocity)\n",
        "        tiles = tc.tiles(self.iht, self.num_tilings, [position * position_scale, velocity * velocity_scale])\n",
        "        return np.array(tiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sarsa Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(w, epsilon, num_actions):\n",
        "    def policy_fn(tiles):\n",
        "        A = np.zeros(num_actions) + 1\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for i in range(num_actions):\n",
        "            action_values[i] = w[i][tiles].sum()\n",
        "        best_action = np.argmax(action_values)\n",
        "        A = A * epsilon/len(A)\n",
        "        A[best_action] += 1 - epsilon\n",
        "        return A, action_values\n",
        "    return policy_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent_Sarsa(AgentBase):\n",
        "    def __init__(self, id, environment, discount, alpha, iht_size, num_tilings, num_tiles):\n",
        "        self.env = environment\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = 0.1\n",
        "        self.discount_factor = discount\n",
        "        self.num_tilings = num_tilings\n",
        "        self.num_tiles = num_tiles\n",
        "        self.iht_size = iht_size\n",
        "        self.num_actions = 3\n",
        "        \n",
        "        self.mct = MountainCarTileCoder(iht_size, num_tilings, num_tiles)\n",
        "\n",
        "        self.initial_weights = np.zeros((self.num_actions, iht_size))\n",
        "        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n",
        "\n",
        "        super(Agent_Sarsa, self).__init__(id, environment)\n",
        "        \n",
        "    def run(self, trail, max_time):\n",
        "        step_episode = []\n",
        "        for i_episode in range(1, trail+1):\n",
        "            self.epsilon *= 0.995\n",
        "            state = self.env.reset()\n",
        "            [position, velocity] = state\n",
        "            active_tiles = self.mct.get_tiles(position, velocity)\n",
        "            behavior_policy = epsilon_greedy_policy(self.w, self.epsilon, self.num_actions)\n",
        "            probs, q_vals = behavior_policy(active_tiles)\n",
        "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            q_val = q_vals[action]\n",
        "\n",
        "            for t in range(max_time):\n",
        "                if(i_episode == trail + 1):\n",
        "                    self.env.render()\n",
        "                #Take action A, observe R,S'\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                [next_position, next_velocity] = next_state\n",
        "                next_active_tiles = self.mct.get_tiles(next_position, next_velocity)\n",
        "\n",
        "                # if S' is terminal\n",
        "                if done:\n",
        "                    q_val = self.w[action][active_tiles].sum()\n",
        "                    delta = reward - q_val\n",
        "                    grad = np.zeros_like(self.w)\n",
        "                    grad[action][active_tiles] = 1\n",
        "                    self.w += self.alpha * delta * grad\n",
        "                    step_episode.append(t)\n",
        "                    print(\"\\rEpisode {}/{} | steps: {} | Epsilon: {} \".format(i_episode, trail, t,self.epsilon ), end=\"\")\n",
        "                    sys.stdout.flush()\n",
        "                    break\n",
        "\n",
        "                #Choose A' as a function of q(s',.w)\n",
        "                behavior_policy = epsilon_greedy_policy(self.w, self.epsilon, self.num_actions)\n",
        "                next_action_probs, next_q_vals = behavior_policy(next_active_tiles)\n",
        "                next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
        "                next_q_val = next_q_vals[next_action]\n",
        "                \n",
        "    \n",
        "                q_val = self.w[action][active_tiles].sum()\n",
        "                delta = reward + self.discount_factor * next_q_val - q_val\n",
        "                grad = np.zeros_like(self.w)\n",
        "                grad[action][active_tiles] = 1\n",
        "                self.w += self.alpha * delta * grad\n",
        "                state = next_state\n",
        "                [position, velocity] = state\n",
        "                active_tiles = self.mct.get_tiles(position, velocity)\n",
        "                action = next_action\n",
        "        return step_episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Run:  1  | Sarsa Agent |  iht_size:  4096  | num_tilings:  32  | num_tiles:  4 \n",
            "\n",
            "Episode 1/100 | steps: 4481 | Epsilon: 0.0995 "
          ]
        }
      ],
      "source": [
        "discount = 0.9\n",
        "trial, max_time = [100,10000]\n",
        "num_runs = 20\n",
        "\n",
        "gym.envs.register(\n",
        "    id='MountainCarVersion-v1',\n",
        "    entry_point='gym.envs.classic_control:MountainCarEnv',\n",
        "    max_episode_steps=max_time, # MountainCar-v0 uses 200\n",
        "    reward_threshold=-1,     \n",
        ")\n",
        "env = gym.make('MountainCarVersion-v1')\n",
        "env.reset()\n",
        "steps_total = []\n",
        "\n",
        "\n",
        "for r in range(num_runs):\n",
        "    \n",
        "    steps_per_episode = []\n",
        "    iht_size, num_tilings, num_tiles = [4096, 2, 16]\n",
        "    clear_output(wait=True)\n",
        "    print('\\nRun: ' , r, ' | Sarsa Agent | ','iht_size: ' ,iht_size, ' | num_tilings: ', num_tilings,' | num_tiles: ' ,num_tiles, '\\n')\n",
        "    alpha = 0.1\n",
        "    agent_sarsa = Agent_Sarsa(0, env, discount, alpha, iht_size, num_tilings, num_tiles)\n",
        "    steps_per_episode.append(agent_sarsa.run(trial, max_time))\n",
        "\n",
        "    iht_size, num_tilings, num_tiles = [4096, 32, 4]\n",
        "    clear_output(wait=True)\n",
        "    print('\\nRun: ' , r, ' | Sarsa Agent | ','iht_size: ' ,iht_size, ' | num_tilings: ', num_tilings,' | num_tiles: ' ,num_tiles, '\\n')\n",
        "    alpha = 0.0005\n",
        "    agent_sarsa = Agent_Sarsa(1, env, discount, alpha, iht_size, num_tilings, num_tiles)\n",
        "    steps_per_episode.append(agent_sarsa.run(trial, max_time))\n",
        "\n",
        "    iht_size, num_tilings, num_tiles = [4096, 8, 8]\n",
        "    clear_output(wait=True)\n",
        "    print('\\nRun: ' , r, ' | Sarsa Agent | ','iht_size: ' ,iht_size, ' | num_tilings: ', num_tilings,' | num_tiles: ' ,num_tiles, '\\n')\n",
        "    alpha = 0.012\n",
        "    agent_sarsa = Agent_Sarsa(2, env, discount, alpha, iht_size, num_tilings, num_tiles)\n",
        "    steps_per_episode.append(agent_sarsa.run(trial, max_time))\n",
        "    steps_total.append(steps_per_episode)\n",
        "\n",
        "plt.plot(np.mean(steps_total,axis=0)[0], label=\"num_tiles: 16, num_tilings: 2\")\n",
        "plt.plot(np.mean(steps_total,axis=0)[1], label=\"num_tiles: 4, num_tilings: 32\")\n",
        "plt.plot(np.mean(steps_total,axis=0)[2], label=\"num_tiles: 8, num_tilings: 8\")\n",
        "plt.legend()\n",
        "plt.title(\"SARSA Agent steps per episode\")\n",
        "plt.ylabel('Steps per Episode')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylim(0, 800)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Question1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
